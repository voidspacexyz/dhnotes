# ## Definitions
### Data, Information, Knowledge, Wisdom (DIKW) Pyramid
![[Pasted image 20241208165459.png]]
- Data is a collection of facts in a raw or unorganized form such as numbers or characters.
- Information is the next building block of the DIKW Pyramid. This is data that has been “cleaned” of errors and further processed in a way that makes it easier to measure, visualize and analyze for a specific purpose.
- “How” is the information, derived from the collected data, relevant to our goals? “How” are the pieces of this information connected to other pieces to add more meaning and value? And, maybe most importantly, “how” can we apply the information to achieve our goal?
- Wisdom is the top of the DIKW hierarchy and to get there, we must answer questions such as ‘why do something’ and ‘what is best’. In other words, wisdom is knowledge applied in action.


## Week Overview
This session will examine the data component of humanities research. In particular, this core topic examines the different uses of the term data asking what, if anything, makes humanities data different. Students will be introduced to the key concepts of data, data cleaning, structured and unstructured data, entities, search engines, indexing, corpus/corpora, and specific kinds of corpora (such as paired corpora for machine translation). We will also begin to consider how databases are structured to support various types of activity.


## Sources
- ### Mandatory 
		**REQUIRED READING, RESEARCH ARTICLES:**  
1. Schöch, C. (2013) Big? Smart? Clean? Messy? Data in the Humanities. Journal of Digital Humanities, 2(3)  [http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/](http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/)  
2.  Ramsey, S. (2004). Databases. In A Companion to Digital Humanities. Blackwell. [https://companions.digitalhumanities.org/DH/?chapter=content/9781405103213_chapter_15.html](https://companions.digitalhumanities.org/DH/?chapter=content/9781405103213_chapter_15.html)

**REQUIRED MATERIALS TO PREPARE:**

1. This Twitter feed brings up a whole host of the issues contributing to why data can be such a problematic concept in the digital humanities.  Take a look and see what kinds of responses you find.  Posner, M. (2018) Humanists out there… if someone out there were to call your sources data, what would your reaction be? Twitter. [https://twitter.com/miriamkp/status/1057706465866133504](https://twitter.com/miriamkp/status/1057706465866133504)
2. There are a few core tools you will want to make sure you know a bit about before this session (hopefully you know some or all of the already!):
    1. First of all, how do spreadsheets (eg MS Excel) differ from Relational Databases (eg MS Access)?  
    2. If you don't know them already, take a look at Open Refine: [https://openrefine.org/](https://openrefine.org/); and Antconc: [https://www.laurenceanthony.net/software/antconc/](https://www.laurenceanthony.net/software/antconc/)  How do tools like this help us to develop a diverse conceptualisation of what humanities data might be?
- ### Further 
	- [Rosenberg, Data Before Fact](https://tcd.blackboard.com/webapps/blackboard/content/listContent.jsp?course_id=_89286_1&content_id=_3175598_1# "Alternative formats")
    
    Rosenberg, D. (2013) Data before the Fact. In Gitelman, L., “Raw Data” Is an Oxymoron. Cambridge: MIT Press, pp. 15-40. (Indeed, all of the essays in this volume are interesting) [http://static1.1.sqspcdn.com/static/f/1133095/23310656/1376447540493/Rosenburg_RawData.pdf](http://static1.1.sqspcdn.com/static/f/1133095/23310656/1376447540493/Rosenburg_RawData.pdf)
    
- ### Borgman, Big Data, Little Data, No Data
    
    [Borgman, Big Data, Little Data, No Data](https://tcd.blackboard.com/webapps/blackboard/content/listContent.jsp?course_id=_89286_1&content_id=_3175598_1# "Alternative formats")
    
    Borgman, C. (2015)  Big Data, Little Data, No Data.  MIT Press.  If you are not minded to read this entire book, the same author has written a number of shorter pieces reflecting on teh nature of data as well, all of which are interesting.  
    
- ### Toth-Czifra, The Risk of Losing Thick Description
    
    [Toth-Czifra, The Risk of Losing Thick Description](https://tcd.blackboard.com/webapps/blackboard/content/listContent.jsp?course_id=_89286_1&content_id=_3175598_1# "Alternative formats")
    
    Somewhat similar to Schöch's piece, this chapter does an excellent job of breaking down the question of what we gain and lose in the preparation of humanistic datasets.
    
    Tóth-Czifra, E. (2020). The Risk of Losing the Thick Description: Data Management Challenges Faced by the Arts and Humanities in the Evolving FAIR Data Ecosystem. In Edmond, J. (ed.) Digital Technology and the Practices of Humanities Research. Cambridge: Open Book Publishers. [https://www.openbookpublishers.com/htmlreader/978-1-78374-839-6/ch10.xhtml#_idTextAnchor219](https://www.openbookpublishers.com/htmlreader/978-1-78374-839-6/ch10.xhtml#_idTextAnchor219)
    
- ### Leonelli, What counts as scientific data?
    
    [Leonelli, What counts as scientific data?](https://tcd.blackboard.com/webapps/blackboard/content/listContent.jsp?course_id=_89286_1&content_id=_3175598_1# "Alternative formats")
    
    One of the problems with humanities datasets is the manner in which they sit on the cusp between cultural heritage collection, identity scripts and structured data collections.  Needless to say they can be all of these things, depending on the purpose to which they are being put.  For our purposes in digital humanities, their ability to function as research data is prime.  In this context, Leonelli's clear-eyed reflection on what research data are can be very instructive.
    
      
    
    Leonelli, S. (2015). What Counts as Scientific Data? A Relational Framework. Philosophy of Science, 82(5), pp. 810-821.  Available at: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4747116/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4747116/)
    
- ### Edmond, What do we mean when we talk about data?
    
    [Edmond, What do we mean when we talk about data?](https://tcd.blackboard.com/webapps/blackboard/content/listContent.jsp?course_id=_89286_1&content_id=_3175598_1# "Alternative formats")
    
    This chapter combines previous work analysing the discourse surrounding data with insights with interview data testing how well understanding of teh term crosses discipline boundaries (answswe: not well).
    
    Edmond, J. 2022.  "What do we mean when we talk about data?" I nEdmond, Horsley, Lehmann and Priddy.  The Trouble with Big Data.  Bloomsbury.  Available at: [https://www.bloomsburycollections.com/monograph-detail?docid=b-9781350239654&pdfid=9781350239654.ch-2.pdf&tocid=b-9781350239654-chapter2](https://www.bloomsburycollections.com/monograph-detail?docid=b-9781350239654&pdfid=9781350239654.ch-2.pdf&tocid=b-9781350239654-chapter2)


### Discussion Questions

 Data is a core element of the technology stack.  Why do many humanists resist this term?  Are their hesitations justified or not?  What kinds of changes does a data-driven paradigm bring to humanities research? Do the tools available for working with data address these concerns?  How do data structures like relational databases work, both technically and as a way to approach 'fascinating problems and intellectual opportunities' (as per Ramsay)?  Why do you think there has been so much buzz around 'big' data, and why are the digital humanities less enamoured of the term, and more inclined toward 'rich' or 'smart' data?  How is this kind of data different?

## Why do many humanists resist the term dtaa

Here are some reasons why humanists resist the term “data”:

- **The term “data” suggests an observer-independent fact that can’t be challenged.** The word comes from the Latin word _datum_, meaning “that which is given”. Humanists see this definition as problematic because it ignores the fact that researchers’ goals, instruments, and perspectives all shape the data they collect.
- **The word “data” implies a passive acceptance of what is given, but in reality, researchers actively construct data.** Humanist Johanna Drucker prefers the word “capta” (“that which has been captured or gathered”) to “data”. Drucker points out that “capturing data is not passively accepting what is given, but actively constructing what one is interested in”.
- **Data are multifaceted objects that can be used as evidence in an argument, but they are not unquestionable facts.** Digital Archivist Trevor Owens argues that data should be treated as:
    - **An artifact**: something that people purposefully create
    - **A text**: something that is open to interpretation
    - **Computer-processable information**: something that can be analyzed quantitatively
- **Humanists are wary of data-driven research because the apparent empiricism of data seems at odds with the principles of humanistic inquiry.** Humanists tend to value context-dependent interpretation and acknowledge the “situated-ness” of researchers, meaning they believe all researchers inevitably bring their own perspectives to their work. They see data as adding another layer of mediation between researchers and their objects of study.

The sources provided focus on explaining differing perspectives on data and the implications of using data within humanistic inquiry. They do not provide any direct evidence suggesting that humanists perceive the term "data" as _sinister_. This information is from outside the provided sources and may require independent verification.

## Smart Data vs. Big Data in the Humanities

The sources describe two primary types of humanities data: **smart data** and **big data**. These data types differ in how they are structured, how “clean” they are, and how large they are.

### Smart Data: Small, Clean, and Explicit

Smart data is:

- **Structured or semi-structured:** This means that relationships between data points are explicitly defined, often using schemas or pre-defined database structures. An example is the Text Encoding Initiative (TEI), which provides guidelines for creating digital editions of texts. Though these digital editions are considered semi-structured because they allow some flexibility, they still make a lot of information explicit that would be implicit in a plain-text version of a work. For example, they identify different parts of a text, like chapters and paragraphs, and they tag specific types of information, like person and place names.
- **Explicit and enriched:** In addition to the core data, smart data includes markup, annotations, and metadata.
- **“Clean”:** This means that any imperfections from the data capture or creation process have been reduced as much as possible.
- **Relatively small in volume:** Creating smart data requires human involvement, which limits its size.

An example of smart data is a database of 1,500 descriptions from 18th-century novels, tagged for dozens of stylistic features, which was used to study how descriptive writing functioned in that era. Creating this database was a time-consuming but valuable process, as it allowed the researcher to uncover recurring configurations, patterns of usage, and trends over time, as well as to account for outliers.

### Big Data: Large, Messy, and Implicit

In contrast to smart data, big data is:

- **Relatively unstructured, messy, and implicit:** Relationships between data points aren’t always clear, and the data often contains errors.
- **Relatively large in volume:** Big data sets can be too large to store on a single computer.
- **Varied in form:** Big data often comes from a variety of sources and is stored in a variety of formats.

Although the size of a big data set is relative and depends on the context and available technology, in general, data is considered “big” when it exceeds the memory capacity of a typical computer, requiring the use of grid computing systems like Hadoop.

The characteristics of big data necessitate a shift from “close reading” to “distant reading” or “macroanalysis”, in which researchers study a collection of hundreds or even thousands of texts at once, rather than focusing on a few select works. Analyzing large quantities of texts requires researchers to:

- **Focus on quantitative measures of low-level features:** Instead of studying literary forms, conventions, and meaning, researchers working with big data might look at word frequency, sentence length, or the presence or absence of specific words or phrases.
- **Analyze entire sets of texts, rather than focusing on those deemed “representative”:** This allows for a more comprehensive understanding of trends in literature and helps researchers move beyond questions of canonization and literary quality.

Despite its potential, big data analysis has its limitations. Even with massive digitization efforts, researchers still don’t have access to a complete record of human culture and creativity. This means that even large digital collections of texts are not truly representative, as they are often “opportunistic” rather than random samples.

### Combining Smart and Big Data

The sources suggest that future research will require combining the strengths of smart and big data to create “**bigger smart data**” or “**smarter big data**”. This could be accomplished through:

- **Automatic annotation:** Using algorithms to identify implicit units, structures, patterns, and relationships within a data set.
- **Crowdsourcing:** Breaking down a large task into smaller units that can be completed by volunteers, then reintegrating the completed units.

The example of **Optical Character Recognition (OCR)** illustrates how automatic annotation and crowdsourcing can work together to create smarter big data. While OCR can convert scanned documents into machine-readable text, it is prone to errors, especially when dealing with print from before 1800 or with handwriting. However, combining OCR with error-detection algorithms and crowdsourcing can lead to more reliable results. The same approach can be applied to automatically annotating text with linguistic features.

Combining smart and big data approaches can help humanities scholars achieve the following:

- **More adequately represent the features of objects of inquiry**
- **Provide enough data to enable robust quantitative analysis**
- **Go beyond the limitations of close reading methods**
- **Enable more intelligent quantitative analysis**

Ultimately, combining big data’s scale with smart data’s depth of information will enable more nuanced and insightful inquiries into literature, history, and culture.

Here are some of the changes a data-driven paradigm brings to humanities research:

- **Shifting research focus from a small number of select works to large collections of texts.** This transition enables "distant reading" or “macroanalysis,” allowing researchers to observe trends across a whole body of literature, potentially mitigating issues related to sampling, representativeness, and canonization.
    
- **Employing quantitative analysis of low-level textual features.** Instead of primarily examining literary forms, conventions, semantics, and context through close reading, researchers engage with quantifiable aspects like word frequency, sentence length, or the presence of specific phrases. This approach helps uncover patterns and trends not easily discernible through traditional methods.
    
- **Reconceptualizing data's role in humanities research.** The sources highlight a nuanced understanding of data. While acknowledging the traditional view of data as observer-independent facts, they emphasize the role of researchers in actively constructing data and underscore the multifaceted nature of data as artifacts, texts, and computer-processable information. This approach challenges the perception of data as objective truth and instead frames it as evidence within a larger argument.
    
- **New methodologies for data creation and analysis.** Researchers are exploring methods like automatic annotation and crowdsourcing to create "smarter big data," or "bigger smart data"
    
    - **Automatic annotation** involves using algorithms to reveal implicit units, structures, and relationships within datasets, making them more readily analyzable.
    - **Crowdsourcing** leverages the collective effort of volunteers to complete smaller tasks, which are then integrated to form a larger, enriched dataset.
- **Necessitating a reassessment of traditional categories in literary history.** Data-driven analysis allows researchers to examine the relationship between established literary categories, such as genres, forms, and periods, and the actual linguistic features present in texts. This can lead to more nuanced understandings of literary conventions and historical trends. For example, the analysis of French plays by Thomas and Pierre Corneille revealed that tragedies exhibited greater stylistic homogeneity compared to comedies, suggesting a stronger adherence to genre conventions in 17th-century French tragedy.
    

However, a data-driven approach also brings challenges:

- **Limited availability of digitized material.** Despite ongoing digitization projects, access to a complete and truly representative record of human cultural output remains a significant obstacle. This limitation impacts the generalizability of findings derived from data analysis.
- **Need for interdisciplinary collaboration.** Bridging the gap between humanities scholars and those with expertise in computer science and data analysis is crucial for the success of data-driven research.
- **Ethical considerations.** As data-driven research becomes more prevalent, issues related to data privacy, ownership, and the potential biases embedded in algorithms need careful consideration. [Please note that this point is based on my general knowledge and is not explicitly addressed in the sources.]
- **Debates about the nature and definition of data in a humanistic context.** The inherent tension between the empirical nature of data and the interpretive focus of humanities research continues to fuel discussions about the most appropriate ways to incorporate data-driven methods into humanistic inquiry.

Overall, the adoption of data-driven approaches represents a significant shift in humanities research. While these methods offer promising avenues for generating new insights and challenging traditional interpretations, they also necessitate careful consideration of their limitations and potential pitfalls. Successful integration of data-driven paradigms requires ongoing dialogue, interdisciplinary collaboration, and a critical reflection on the nature and role of data in shaping our understanding of human culture and expression.

## Addressing Concerns about Data-Driven Humanities Research

The sources address the effectiveness of current tools in addressing concerns about data-driven humanities research, particularly in terms of creating and analyzing smarter, larger datasets, and integrating quantitative methods into humanistic inquiry.

### Tools for Creating and Analyzing Data

**1. Automatic Annotation:**

- Automatic annotation utilizes algorithms to identify implicit units, structures, and relationships within datasets, enhancing their analyzability.
- The sources present **Optical Character Recognition (OCR)** as a prime example of combining automatic annotation and crowdsourcing.
- While OCR can convert scans into machine-readable text, it is error-prone, especially with older materials or handwriting.
- The use of error-detection algorithms, coupled with crowdsourced correction mechanisms like **Captcha**, significantly improves OCR reliability.
- Similarly, automatic linguistic annotation, even for well-studied languages, requires careful scrutiny and correction, suggesting the need for more advanced tools and collaborative efforts to enhance the reliability of large-scale annotation projects.

**2. Crowdsourcing:**

- Crowdsourcing involves breaking down large tasks into smaller, manageable units that volunteers can complete, which are then integrated to create a richer dataset.
- This approach proves valuable for tasks like text transcription and annotation, as it leverages collective effort to tackle projects that would be impractical for individual researchers or small teams.
- Success in crowdsourcing hinges on effective task division, motivational strategies like gamification, and efficient mechanisms for data reintegration.

**3. Specialized Tools and Databases:**

- The sources highlight tools like **NoSQL** databases and graph-based databases as solutions for handling the heterogeneity and unstructured nature of much humanities data.
- These tools offer flexibility compared to traditional **SQL** databases and facilitate the analysis of complex, interconnected datasets.
- The **Text Encoding Initiative (TEI)** provides guidelines for creating structured, annotated digital editions of texts, exemplified by projects like **TextGrid**'s Digital Library and the **Théâtre Classique** collection.
- **Scholarly digital editions** and annotated linguistic corpora provide researchers with rich, contextualized data, although they tend to be smaller in scale due to the manual labor involved in their creation.

### Addressing Methodological and Epistemological Concerns

**1. Shifting from Close to Distant Reading:**

- The sources advocate for a shift from close reading to distant reading or macroanalysis, enabling the analysis of vast collections of texts to reveal large-scale patterns and trends.
- Tools like **principal component analysis** and **cluster analysis**, as demonstrated in the analysis of French drama, allow researchers to investigate relationships between literary categories and linguistic features across hundreds of plays.

**2. Reconceptualizing Data in the Humanities:**

- The sources acknowledge the limitations of viewing data as objective, observer-independent facts.
- They propose a more nuanced understanding of data as constructed artifacts, subject to interpretation and shaped by the contexts of their creation and use.
- This reframing encourages researchers to critically assess data sources, recognize potential biases, and consider the limitations of data-driven approaches.

### Limitations and Future Directions

While current tools offer significant advancements, the sources acknowledge the need for further development.

**1. Data Availability:**

- Despite ongoing digitization efforts, the lack of complete, representative digital collections remains a significant challenge.
- This limitation restricts the generalizability of findings based on available datasets and highlights the need for continued efforts to expand digital archives.

**2. Smarter Big Data:**

- The sources emphasize the need to develop "smarter big data" or "bigger smart data" to combine the scale of big data with the richness of contextual information found in smart data.
- This requires further development of automatic annotation and crowdsourcing methods to create larger, more thoroughly annotated and structured datasets.

**3. Ethical Considerations:**

- While not explicitly addressed in the provided sources, the ethical implications of data-driven research in the humanities, including data privacy, ownership, and algorithmic bias, require ongoing attention and careful consideration. This is based on my general knowledge and may require independent verification.


## Exploring Relational Databases: Technical Functioning and Intellectual Opportunities

While the sources don't directly address the technical specifics of relational databases, they provide valuable insights into how data structures, including databases, shape approaches to research questions and open up new avenues for exploration in the humanities.

**What are Relational Databases?**

Although not explicitly defined in the sources, relational databases are a fundamental data structure in computing, often employed in humanities research. **A relational database organizes data into tables with rows and columns.** Each row represents a unique record, and each column represents a specific attribute of that record. The power of relational databases lies in their ability to establish **relationships between different tables** based on shared attributes, enabling complex queries and analysis across multiple datasets.

**Connecting Relational Databases to the Sources:**

The sources discuss various data structures and their implications for humanities research, highlighting the need for tools capable of handling:

- **Structured Data:** Relational databases excel at managing structured data, aligning with the concept of "smart data" emphasized in. "Smart data" is characterized by its organization, explicitness, and enrichment through markup, annotations, and metadata, features well-suited for storage and retrieval in relational databases.
- **Relationships between Data:** The sources stress the importance of understanding relationships between data points. describes data as "a relational category" emphasizing its interconnectedness, aligning with the core functionality of relational databases in linking tables based on shared attributes.
- **Heterogeneity of Data:** The sources acknowledge the challenges posed by the variety of data formats and sources encountered in humanities research. This heterogeneity is where relational databases, particularly in conjunction with tools like NoSQL databases, provide flexibility in accommodating diverse data types while maintaining structured relationships between them.

**Relational Databases as a Tool for Inquiry:**

Ramsay's notion of 'fascinating problems and intellectual opportunities' can be linked to how relational databases empower researchers to:

- **Ask Complex Questions:** By establishing relationships between tables, researchers can go beyond simple queries about individual records and explore intricate connections across datasets. For instance, a database of literary texts linked to biographical information about authors could facilitate investigations into the influence of personal experiences on writing style or thematic choices.
- **Uncover Hidden Patterns:** The ability to perform sophisticated queries and analysis on interconnected data allows for the discovery of patterns and trends that might remain hidden through traditional close reading methods.
- **Re-examine Existing Categories:** Relational databases can help challenge and refine established categories in literary history. For example, a database linking texts with linguistic features and genre classifications could reveal subtle variations within genres or unexpected overlaps between seemingly distinct categories.

**Limitations of Relational Databases:**

It is important to note that while relational databases are powerful tools, they have limitations:

- **Complexity of Implementation:** Designing and managing relational databases effectively requires technical expertise, potentially posing a barrier for humanities scholars without a strong computational background.
- **Potential for Bias:** The structure imposed by relational databases can introduce biases, particularly in how categories are defined and relationships are established. Researchers must remain vigilant about these potential biases and critically evaluate the assumptions embedded in their database design.

## The Perils of Decontextualized Data

The sources highlight multiple challenges that stem from the lack of context in data, particularly within humanities research. This absence of context can distort interpretations, limit the usefulness of data as evidence, and obscure the human processes behind data creation and curation.

- **Misinterpreting Data**: When data is stripped of its original context, it becomes susceptible to misinterpretations. A Google search on suicide methods could be driven by genuine distress or simply research for a writing project. Similarly, visiting a specific location could be for various reasons like walking a dog, buying a doughnut, or even escaping a perceived threat. Without understanding the motivations and circumstances behind the data, drawing accurate conclusions becomes challenging.
- **Diminished Value as Evidence**: The sources, particularly, stress that data divorced from its context loses much of its value as evidence. Data's significance is inherently tied to the specific inquiry it aims to address. Without knowing how data was collected, who collected it, for what purpose, and what transformations it underwent during its "journey," it becomes difficult to assess its reliability and relevance to a new research question. This is explicitly recognized in the NASA data complexity model, where changing the context of data use also changes its complexity, regardless of prior enhancements.
- **Overlooking Human Influence**: One of the central concerns raised is the tendency to view data as objective and detached from human influence. Terms like "data" (meaning "given") can perpetuate this misconception. The sources argue that data is always "capta," meaning "taken" or constructed through human choices and actions. These choices, influenced by researchers' backgrounds, intentions, and available tools, shape the data collected and ultimately determine its meaning. Neglecting this human element can lead to a false sense of objectivity and obscure potential biases embedded in the data.

**Specific Consequences of Lack of Context**

The sources provide concrete examples of how decontextualized data can create problems in research:

- **Difficulty in Defining Data**: The lack of a stable, context-independent definition of data poses a challenge for discussions about data-driven research. This ambiguity is evident in the various attempts to define "big data," with differing interpretations arising from industry, media, and academia. The sources argue that defining data requires acknowledging its relational nature and understanding how its meaning shifts depending on the research context.
- **The Problem of "Data Cleaning":** The process of "data cleaning" or "data manipulation" highlights the tensions between maintaining data integrity and adapting it for specific research goals. Without a clear understanding of the original context and the rationale behind data cleaning decisions, it becomes challenging to assess the impact of these manipulations on the data's meaning and reliability.
- **"Context Collapse" and the Limits of Generalizability**: The phenomenon of "context collapse," where information intended for a specific audience becomes accessible to a broader, unintended audience, illustrates the dangers of ignoring context. In a data-driven environment, failing to account for the original context of data can lead to inappropriate generalizations and misinterpretations, particularly when analyzing human behavior or social interactions.

**Addressing the Challenges**

The sources suggest several strategies for mitigating the challenges posed by decontextualized data:

- **Emphasizing Provenance and Curation**: Documenting the origins of data, including its collection methods, purpose, and any transformations it underwent, is crucial for maintaining context. This information, often referred to as data provenance, allows researchers to critically evaluate the data's suitability for their research questions. Additionally, transparent and well-documented data curation practices that consider the ethical implications of data use can help ensure data integrity and minimize the risks of misinterpretation.
- **Moving Towards "Smarter Big Data":** The sources advocate for the development of "smarter big data" or "bigger smart data," combining the scale of large datasets with the richness of contextual information found in carefully curated, annotated datasets. This integration requires advancements in automatic annotation, crowdsourcing methods, and data-linking technologies to create more comprehensive and nuanced datasets that retain essential context.
- **Adopting a Critical Perspective on Data**: Recognizing the constructed nature of data and understanding how human choices influence its creation is essential for responsible data-driven research in the humanities. By acknowledging that data is not simply "given" but actively "taken," researchers can develop a more critical perspective on data sources, identify potential biases, and avoid oversimplifications that arise from neglecting context.

**Beyond the Sources:**

The concept of "data storytelling" is gaining traction in data science and visualization. While not explicitly mentioned in the sources, it could be a valuable approach for addressing the challenges of decontextualized data. Data storytelling goes beyond simply presenting numbers and graphs; it aims to provide context, narrative, and human meaning to data, making it more engaging and understandable. Applying this approach to humanities research could help bridge the gap between quantitative methods and the interpretive traditions of the field. This information is based on my general knowledge and may require independent verification.

In conclusion, the sources highlight the crucial role of context in data-driven research, particularly within the humanities. By acknowledging the limitations of decontextualized data and adopting strategies to preserve and enrich data with contextual information, researchers can harness the power of quantitative methods while maintaining the critical and interpretive rigor essential to humanistic inquiry.